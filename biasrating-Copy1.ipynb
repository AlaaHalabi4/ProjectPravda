{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9379ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip installs:\n",
    "\n",
    "# pip install requests\n",
    "# pip install beautifulsoup4\n",
    "# pip install lxml\n",
    "# pip install rich\n",
    "\n",
    "#pip install selenium\n",
    "\n",
    "\n",
    "#in a jupyter console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2a62c",
   "metadata": {},
   "source": [
    "# The ratio of the community ratings turned to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc844b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# js file https://www.allsides.com/sites/default/files/js/js_RkhuUtuGis_wrzoAtkhY74JegSNy1LhoCkjSMXavCTI.js\n",
    "\n",
    "def community_vote(ratio):\n",
    "    # the value of the ratio goes through this if statement and prints out the return string\n",
    "    if ratio > 3:\n",
    "        return \"Absolutely Agree\"\n",
    "\n",
    "    elif 2 < ratio <= 3:\n",
    "        return \"Strongly Agree\"\n",
    "\n",
    "    elif 1.5 < ratio <= 2:\n",
    "        return \"Agree\"\n",
    "\n",
    "    elif 1 < ratio <= 1.5:\n",
    "        return \"Somewhat Agree\"\n",
    "\n",
    "    elif 0.67 < ratio < 1:\n",
    "        return \"Somewhat Disagree\"\n",
    "\n",
    "    elif 0.5 < ratio <= 0.67:\n",
    "        return \"Disagree\"\n",
    "\n",
    "    elif 0.33 < ratio <= 0.5:\n",
    "        return \"Strongly Disagree\"\n",
    "\n",
    "    elif ratio <= .33:\n",
    "        return \"Absolutely Disagree\"\n",
    "\n",
    "    elif ratio == 1:\n",
    "        return \"Neutral\"\n",
    "\n",
    "    else:\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e63cf",
   "metadata": {},
   "source": [
    "# Code without infinite scroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfccb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraper is parsing the table!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7f842d91a747068851f912cedd00ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing has finished\n",
      "None\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#from communityfeedback import *\n",
    "from time import sleep\n",
    "from rich.progress import track\n",
    "import json\n",
    "\n",
    "# TODO: Finish progress bar. \n",
    "# TODO: For numbers that are over 10,000 add the comma. \n",
    "# TODO: For Allsides Bias Rating change 'left-center' --> 'Left Center'. \n",
    "\n",
    "\n",
    "def table(full_table):\n",
    "    print('Web scraper is parsing the table!')\n",
    "    # only collects data from the first table that shows not the ones that load after the infinite scroll.\n",
    "    for url in ['https://www.allsides.com/media-bias/ratings?field_featured_bias_rating_value=All&field_news_source_type_tid[1]=1&field_news_source_type_tid[2]=2&field_news_source_type_tid[3]=3&field_news_source_type_tid[4]=4']:\n",
    "        source = requests.get(url)\n",
    "        soup = BeautifulSoup(source.content, 'lxml')\n",
    "        \n",
    "        main_table = soup.select('tbody tr')\n",
    "        global count\n",
    "        count=0\n",
    "        \n",
    "\n",
    "        for row in main_table:\n",
    "            \n",
    "            f = dict()\n",
    "\n",
    "            f['NewsSource'] = row.select_one('.source-title').text.strip()\n",
    "            f['AllSidesBiasRating'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1].replace(\"-\",\" \")\n",
    "            f['NewsMediaInfo'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']\n",
    "            f['Agree'] = int(row.select_one('.agree').text)\n",
    "            f['Disagree'] = int(row.select_one('.disagree').text)\n",
    "            f['Ratio'] = (f['Agree'] / f['Disagree'])\n",
    "            f['CommunityFeedback'] = community_vote(f['Ratio'])\n",
    "            f['Ratio'] = '{:.3f}'.format(f['Ratio'])\n",
    "            \n",
    "            count=count+1\n",
    "              \n",
    "\n",
    "            full_table.append(f)  # adds it to the empty list\n",
    "\n",
    "        sleep(3)  # this is due to the ten seconds before request in robots.txt\n",
    "    return full_table\n",
    "\n",
    "def website(full_table):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    for f in track(full_table, description='Parsing...'):\n",
    "        # Initialize default values\n",
    "        f['Bias Rating'] = 'N/A'\n",
    "        f['Type'] = 'N/A'\n",
    "        f['Region'] = 'N/A'\n",
    "        f['Owned By'] = 'N/A'\n",
    "        f['Established'] = 'N/A'\n",
    "        f['Website'] = 'N/A'\n",
    "        f['Twitter'] = 'N/A'\n",
    "        f['Facebook'] = 'N/A'\n",
    "        f['Wikipedia'] = 'N/A'\n",
    "\n",
    "        response = requests.get(f['News Media Info'], headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        source_details_container = soup.find('div', class_='info-table_container')\n",
    "        if not source_details_container:\n",
    "            print(f\"Details not found for {f['News Source']}\")\n",
    "            continue\n",
    "\n",
    "        info_table_rows = source_details_container.find_all('tr')\n",
    "\n",
    "        for row in info_table_rows:\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 2:\n",
    "                label = columns[0].text.strip()\n",
    "                value_content = columns[1]\n",
    "\n",
    "                # Extract URL if available, otherwise text\n",
    "                link = value_content.find('a')\n",
    "                if link and 'href' in link.attrs:\n",
    "                    value = link['href'] if link['href'].startswith('http') else link.text\n",
    "                else:\n",
    "                    value = value_content.text.strip()\n",
    "\n",
    "                # Map the label to the corresponding field in your dictionary\n",
    "                if label == 'Bias Rating':\n",
    "                    f['Bias Rating'] = value\n",
    "                elif label == 'Type':\n",
    "                    f['Type'] = value\n",
    "                elif label == 'Region':\n",
    "                    f['Region'] = value\n",
    "                elif label == 'Owner':\n",
    "                    f['Owned By'] = value\n",
    "                elif label == 'Established':\n",
    "                    f['Established'] = value\n",
    "                elif label == 'Website':\n",
    "                    f['Website'] = value\n",
    "                elif label == 'Twitter':\n",
    "                    f['Twitter'] = value\n",
    "                elif label == 'Facebook':\n",
    "                    f['Facebook'] = value\n",
    "                elif label == 'Wikipedia':\n",
    "                    f['Wikipedia'] = value\n",
    "\n",
    "    # Adjust sleep as per necessity and compliance with robots.txt\n",
    "    sleep(2)\n",
    "    print('Parsing has finished')\n",
    "    return full_table\n",
    "\n",
    "\n",
    "def saving_data(full_table):\n",
    "    # saves the data into a json file with no lines\n",
    "    with open('all-sidesTEST.json', 'w') as i:\n",
    "        yo = json.dump(full_table, i, indent=4)\n",
    "        print(yo)\n",
    "        print(count)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #global full_table\n",
    "    full_table = []  # empty list\n",
    "    full_table = table(full_table)\n",
    "    #print(full_table)\n",
    "    full_table = website(full_table)\n",
    "    saving_data(full_table)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdc87e",
   "metadata": {},
   "source": [
    "# THE CODE: (infinite scroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03cca8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraper is parsing the table!\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n",
      "Error processing row: division by zero\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98c4e1f02b74dd1a797bb327e003edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing has finished\n",
      "2278\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "from rich.progress import track\n",
    "import json\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = Options()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def scroll_to_bottom(driver):\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(3)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def table(full_table):\n",
    "    print('Web scraper is parsing the table!')\n",
    "\n",
    "    url = 'https://www.allsides.com/media-bias/ratings?field_featured_bias_rating_value=All&field_news_source_type_tid[1]=1&field_news_source_type_tid[2]=2&field_news_source_type_tid[3]=3&field_news_source_type_tid[4]=4'\n",
    "    driver = init_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    scroll_to_bottom(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    main_table = soup.select('tbody tr')\n",
    "    \n",
    "    global count\n",
    "    count=0\n",
    "\n",
    "    for row in main_table:\n",
    "        f = dict()\n",
    "        \n",
    "        try:\n",
    "            f['News Source'] = row.select_one('.source-title').text.strip()\n",
    "            f['AllSides Bias Rating'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1].replace(\"-\",\" \")\n",
    "            f['News Media Info'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']\n",
    "            f['Agree'] = int(row.select_one('.agree').text)\n",
    "            f['Disagree'] = int(row.select_one('.disagree').text)\n",
    "            f['Ratio'] = (f['Agree'] / f['Disagree'])\n",
    "            f['Community Feedback'] = community_vote(f['Ratio'])\n",
    "            f['Ratio'] = '{:.3f}'.format(f['Ratio'])\n",
    "            \n",
    "            count=count+1\n",
    "            \n",
    "            # Uncomment the following line if community_vote function is available\n",
    "             #f['Community Feedback'] = community_vote(float(f['Ratio']))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            continue\n",
    "\n",
    "        full_table.append(f)\n",
    "\n",
    "    driver.quit()\n",
    "    return full_table\n",
    "\n",
    "def website(full_table):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    #for f in track(full_table[5], description=\"parsing...\"):\n",
    "    for f in track(full_table, description='Parsing...'):\n",
    "        # Initialize default values\n",
    "        f['Bias Rating'] = 'N/A'\n",
    "        f['Type'] = 'N/A'\n",
    "        f['Region'] = 'N/A'\n",
    "        f['Owned By'] = 'N/A'\n",
    "        f['Established'] = 'N/A'\n",
    "        f['Website'] = 'N/A'\n",
    "        f['Twitter'] = 'N/A'\n",
    "        f['Facebook'] = 'N/A'\n",
    "        f['Wikipedia'] = 'N/A'\n",
    "\n",
    "        response = requests.get(f['News Media Info'], headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        source_details_container = soup.find('div', class_='info-table_container')\n",
    "        if not source_details_container:\n",
    "            print(f\"Details not found for {f['News Source']}\")\n",
    "            continue\n",
    "\n",
    "        info_table_rows = source_details_container.find_all('tr')\n",
    "\n",
    "        for row in info_table_rows:\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 2:\n",
    "                label = columns[0].text.strip()\n",
    "                value_content = columns[1]\n",
    "\n",
    "                # Extract URL if available, otherwise text\n",
    "                link = value_content.find('a')\n",
    "                if link and 'href' in link.attrs:\n",
    "                    value = link['href'] if link['href'].startswith('http') else link.text\n",
    "                else:\n",
    "                    value = value_content.text.strip()\n",
    "\n",
    "                # Map the label to the corresponding field in your dictionary\n",
    "                if label == 'Bias Rating':\n",
    "                    f['Bias Rating'] = value\n",
    "                elif label == 'Type':\n",
    "                    f['Type'] = value\n",
    "                elif label == 'Region':\n",
    "                    f['Region'] = value\n",
    "                elif label == 'Owner':\n",
    "                    f['Owned By'] = value\n",
    "                elif label == 'Established':\n",
    "                    f['Established'] = value\n",
    "                elif label == 'Website':\n",
    "                    f['Website'] = value\n",
    "                elif label == 'Twitter':\n",
    "                    f['Twitter'] = value\n",
    "                elif label == 'Facebook':\n",
    "                    f['Facebook'] = value\n",
    "                elif label == 'Wikipedia':\n",
    "                    f['Wikipedia'] = value\n",
    "\n",
    "    # Adjust sleep as per necessity and compliance with robots.txt\n",
    "    time.sleep(2)\n",
    "    print('Parsing has finished')\n",
    "    return full_table\n",
    "\n",
    "\n",
    "def saving_data(full_table):\n",
    "    # saves the data into a json file\n",
    "    with open('all-sidesFINAL.json', 'w') as i:\n",
    "        json.dump(full_table, i, indent=4)\n",
    "        print(count)\n",
    "\n",
    "def main():\n",
    "    #global full_table\n",
    "    full_table = []  \n",
    "    full_table = table(full_table)\n",
    "    full_table = website(full_table)\n",
    "    saving_data(full_table)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e943f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
